{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2023-12-13 ITHS\n",
    "\n",
    "```\n",
    "===============================\n",
    "\n",
    "Lektion 11: Robert Nyquist\n",
    "\n",
    "===============================\n",
    "```\n",
    "\n",
    "### NLP & RNN\n",
    "\n",
    "[wikipedia.org/wiki/Natural_language_processing](https://en.wikipedia.org/wiki/Natural_language_processing)<br>\n",
    "[wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "- Stop words\n",
    "- Lemmatization & Stemming\n",
    "- Word embeddings\n",
    "<br><br>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP combines linguistics and AI.\n",
    "- Natural language much complex, computadora no comprendo\n",
    "\n",
    "Many difficulties, not just for computers:\n",
    "- Different meaning (Polysemy, Homophones, Idioms, Sarcasm & Irony)\n",
    "- Different languages (False friend)\n",
    "- Culural and historical context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting topics on language ambiguity\n",
    "\n",
    "[Polysemy](https://en.wikipedia.org/wiki/Polysemy) | [Aberrant decoding](https://en.wikipedia.org/wiki/Aberrant_decoding) | [False friend](https://en.wikipedia.org/wiki/False_friend) | [Dysphemism](https://en.wikipedia.org/wiki/Dysphemism) | [Oxymoron](https://en.wikipedia.org/wiki/Oxymoron) | [Homophone](https://en.wikipedia.org/wiki/Homophone) | [Phrasal verbs](https://en.wikipedia.org/wiki/English_phrasal_verbs)\n",
    "\n",
    "How would you learn a computer to deal with this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a *good* text?\n",
    "- Text can vary in quality.\n",
    "- How to deal with spelling and grammar?\n",
    "\n",
    "Language changes quickly over time.\n",
    "\n",
    "**Text corpus**: Dataset with text (large, unstructured)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Trying to learn the *meaning* of the words in a sentence by breaking up all the words.\n",
    "\n",
    "Sometimes useful with placeholder for unknown words. \"UNK\" or `<UNK>` (Unknown word or unknown token).\n",
    "\n",
    "```\n",
    "{\n",
    "    \"<UNK>\":0,\n",
    "    \"jag\":1,\n",
    "    \"läser\",2,\n",
    "    \"en\":3,\n",
    "    \"bok\":4,\n",
    "}\n",
    "```\n",
    "\n",
    "[N-grams](https://en.wikipedia.org/wiki/N-gram) to break sentences into sequences with n words.\n",
    "\n",
    "1-gram (unigram): \"Jag\", \"läser\", \"en\", \"bok\"<br>\n",
    "2-gram (bigram) : \"Jag läser\", \"läser en\", \"en bok\"<br>\n",
    "3-gram (trigram): \"Jag läser en\", \"läser en bok\"\n",
    "\n",
    "For sentiment analysis: use tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "\"and\", \"is\", \"the\", \"in\", \"to\", \"it\n",
    "\n",
    "Very frequent words, often not very useful (english and swedish). *Can* be very important, but generally recommended to remove.\n",
    "\n",
    "Removing stopwords:\n",
    "- Reduce dimensions in out data\n",
    "- Remove noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization & Stemming\n",
    "\"Normalizing\" words in terms of their grammatical root or base form.\n",
    "\n",
    "[Stemming](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html): Remove inflectional endings.\n",
    "\n",
    "Example:\n",
    "\n",
    "Stemming would reduce \"running\", \"ran\", and \"runner\" into \"run\".<br>\n",
    "Lemmatization would identify the **base form** of each word based on **context**.\n",
    "\n",
    "The complexity of the language can make lemmatization quite difficult.\n",
    "\n",
    "Why lemmatization?\n",
    "- Reduce risk of overfitting\n",
    "- Reduce dimensionality\n",
    "- Better word count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "We cannot count word (strings).\n",
    "\n",
    "\"House\" och \"cottage\" are very similar to us native NL-speakers. But there is no similarity in the letters.\n",
    "\n",
    "To combat this we translate the strings to vectors.\n",
    "\n",
    "Example: [Word2Vec](https://en.wikipedia.org/wiki/Word2vec) \n",
    "\n",
    "King - Man + Woman = Queen\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:2000/format:webp/1*SYiW1MUZul1NvL1kc1RxwQ.png)\n",
    "\n",
    "From [TowardsDataScience: A Guide to Word Embedding](https://towardsdatascience.com/a-guide-to-word-embeddings-8a23817ab60f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding\n",
    "When working with sentences we won't know the *size of our input*.<br>\n",
    "All sentces will be the same length, set a threshold on max: Padding\n",
    "\n",
    "Padding adds extra tokens to a sequence of data to make it the same length as other sequences in a dataset.\n",
    "\n",
    "Example: 5 input token:\n",
    "```\n",
    "{\n",
    "    \"<PAD>\":0,\n",
    "    \"<UNK>\":1,\n",
    "    \"jag\":2,\n",
    "    \"läser\":3,\n",
    "    \"en\":4,\n",
    "    \"bok\":5\n",
    "}\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reccurent Neural Network\n",
    "\n",
    "RNN handles sequences of data, where **order matters**.\n",
    "\n",
    "\"Student cook delicious food\" != \"Delicious food cook student\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN_1.png](../Resources/material/RNN_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X_t$: Input<br>\n",
    "$h_t$: Hidden state (memory)<br>\n",
    "$A$: Activation function<br>\n",
    "\n",
    "$h_t=\\sigma(W_{hh}h_{t-1} + W_{xh}x_t+b_h)$\n",
    "\n",
    "$y_t = W_{hy}h_t + b_y$\n",
    "\n",
    "Memory handles sequence internally in the layers. <br>\n",
    "With the hidden state, we use weights * previous neuron in the same layer + weight * input + bias => hidden state (memory) -> Activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular backpropagation calculates the gradients of the weights given an error.\n",
    "The error is the difference in output and the ground truth. => Get gradient.\n",
    "\n",
    "This needs adjustments when working with RNNs.\n",
    "\n",
    "## Backpropagation through time (BPTT)\n",
    "Backpropagation for RNN, [BPTT](https://en.wikipedia.org/wiki/Backpropagation_through_time)\n",
    "\n",
    "[RNN Unrolling](https://machinelearningmastery.com/rnn-unrolling/)\n",
    "\n",
    "We calculate gradient at every time step.<br>\n",
    "Sum gradient for all time step and update weights.\n",
    "\n",
    "At a time step `t` we calculate the error `et`\n",
    "\n",
    "1. We unroll the RNN network in time by creating a new network for each time step of the input sequence. Each time step the network has the same structure as the original network, but it only has input from the current time step and output from the previous time step.\n",
    "\n",
    "2. We apply backpropagation to each time step network. We calculate the error for each time step and update the weights accordinly.\n",
    "\n",
    "3. We sum up the gradients for all time steps. This gives us the final gradient for the entire input sequence.\n",
    "\n",
    "4. We use the final gradient to update the weights of the original network.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder\n",
    "\n",
    "Sequence-to-sequence\n",
    "\n",
    "[seq2seq](https://en.wikipedia.org/wiki/Seq2seq)\n",
    "\n",
    "Handle dynamic length of output.\n",
    "\n",
    "Encoder-Decoder architecture\n",
    "\n",
    "- Encoder: Sequence ➜ Fixed length output\n",
    "- Decoder: Output ➜ Dynamic length ouput\n",
    "\n",
    "I am a student ➜ Encoder ➜ [0.34, 1.23][0.34, 1.23] ➜ Decoder ➜ soy un estudiante\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series\n",
    "\n",
    "<center>\n",
    "<blockquote><h3>If we cannot randomize order, we should consider RNNs.</h3></blockquote>\n",
    "</center>\n",
    "\n",
    "Order matters!\n",
    "\n",
    "- Weather\n",
    "- Stocks\n",
    "- Health data etc.\n",
    "\n",
    "\n",
    "Time series:\n",
    "- Feature engineering\n",
    "- Handle missing data\n",
    "- Normalization etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "RNN: Long Short-Term Memory\n",
    "\n",
    "![LSTM_1.png](../Resources/material/LSTM_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'lotta shit goin' on. Instead of just one activation:<br>\n",
    "Input, hidden state, memory, forget gate, input gate, candidate memory, output gate.\n",
    "\n",
    "- Memory (Cell state): Memory\n",
    "- Candidate memory: Contain all information from input\n",
    "- Forget gate: What data to discard\n",
    "- Input gate: What data should be added to cell state\n",
    "- Output gate: What data should be used to calculate output\n",
    "\n",
    "\n",
    "Flow:\n",
    "\n",
    "- Three inputs: $x_t$ (words in a sentence), $h_{t-1}$ (previous hidden state) and previous cell state ($C_{t-1}$)\n",
    "\n",
    "- Forget gate decides what information to store in the cell. Previous hidden state and input to generte a vector in range 0,1. (0 = forget everything, 1 = store everything)\n",
    "\n",
    "- Input gate decides what values to update in cell.\n",
    "\n",
    "- Combining informations from forget gate and input gate: Cell decides what to add or update.\n",
    "\n",
    "- Ouput cell + new hidden state used in next cell.\n",
    "\n",
    "\n",
    "Pros & Cons\n",
    "\n",
    "- **Handling long-term dependencies** in time, word, sentences, etc.\n",
    "- **Improved complexity** (slower train and predict)\n",
    "- **Vulnearbility to overfitting** especially when the dataset is limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU\n",
    "\n",
    "Gated Recurrent Unit\n",
    "\n",
    "![GRU_1.png](../Resources/material/GRU_1.png)\n",
    "\n",
    "###### *\"lite mer lagom\"*\n",
    "\n",
    "Less complex. Easier to train. Cannot handle complex problems as well as LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flow:\n",
    "\n",
    "- Two input $x_t$ (word in a sentence) and $h_{t-1}$ (previous hidden state). Like RNN.\n",
    "- Reset gate decides how much previous hidden state should be combined with current input.\n",
    "- Update gate decides how much of the cell's output should retain new information (current input) and how much to keep previous hidden state.\n",
    "- Hidden state output: Combination of input and reset gate + update gate output"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
