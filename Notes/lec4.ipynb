{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2023-11-27 ITHS\n",
    "\n",
    "```\n",
    "===============================\n",
    "\n",
    "Lektion 4: Robert Nyquist\n",
    "\n",
    "===============================\n",
    "```\n",
    "\n",
    "- Vanishing Gradient problem\n",
    "- Gradient Descent\n",
    "- Optimization algos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient problem\n",
    "```\n",
    "Too small = Vanishing gradient\n",
    "Too large = Exploding gradient\n",
    "```\n",
    "[wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "\n",
    "**Vanishing**: occurs when the gradients become too small during training, hindering the model from updating its weights efficiently.\n",
    "\n",
    "**Exploding**: occurs when the gradients become too large during training, causing the model to update its weights excessively.\n",
    "\n",
    "TODO when vanishing:\n",
    "- Change activation func (to ReLU)\n",
    "- Reduce the network size\n",
    "- Change learning rate or optimization algo\n",
    "\n",
    "TODO when exploding:\n",
    "- Use L2 regularization (Ridge Regression)\n",
    "- Reduce NN depth\n",
    "- Reuce learnig rate\n",
    "- Use gradient clipping (cap max val)\n",
    "\n",
    "!! More on vanishing gradient problem later.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One epoch = One iteration through all data (all batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "- An optimization algo used to minimize a function, (loss or cost).\n",
    "- Iteratively moves towards the *minimum of the function* by updating weights.\n",
    "\n",
    "**GD**\n",
    "- Batch Gradient Gescent\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- Mini-batch Stocastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch GD\n",
    "- Mean gradient for all training data\n",
    "- Stable updates / smooth convergence\n",
    "- Performs well on smaller data\n",
    "- Kind of computatiomnal heavy (CPU/GPU intensitive)\n",
    "- Takes lot of RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Batch GD\n",
    "- Process one data point at a time\n",
    "- Fast iterations\n",
    "- Performs well on big data\n",
    "- High variance between iterations\n",
    "- Might converge slower than Batch GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic mini-batch GD\n",
    "- Combination of Batch GD and SGD\n",
    "- n data points for each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optimization algos\n",
    "- Momentum\n",
    "- Adam\n",
    "- RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "- Adds a histoty parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp (Root Mean Square Propagation)\n",
    "- Adaptive learning rate\n",
    "- Reduce steps for large gradients\n",
    "- Increase steps for small gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "- Kind of cpu efficient\n",
    "- Might overfit small datasets"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
