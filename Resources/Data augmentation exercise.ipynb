{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79b18f4-85df-4912-a4e3-4e0367dcde89",
   "metadata": {},
   "source": [
    "# Data augmentation exercise\n",
    "In this exercise we'll practice data augmentation on images. This includes:\n",
    "* Translation\n",
    "* Mirroring\n",
    "* Zoom\n",
    "* Rotation\n",
    "\n",
    "And to make this a bit more challenging we'll also have a bounding box (used often for object detection or image classification with localization) that also will have to be adjusted to fit the augmented image. \n",
    "\n",
    "In this exercise I've prepared to use PIL (Python Image Library) but other libraries can be used if you prefer. OpenCV is a good alternative with lots of good tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1452eb-7510-44ac-8060-3f0cc9af219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45e89f-c1f3-4023-aba9-c8b66bdef476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "path = \"material/casper_l√•da.jpg\" # Use your own image here if you'd like :) \n",
    "image = Image.open(path).rotate(0).resize((150,150)) \n",
    "\n",
    "# Define a sample bounding box\n",
    "bbox = [16, 57, 58, 42]  # Format: [x_min, y_min, width, height]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e571df80-f958-4967-8391-615691abe4ce",
   "metadata": {},
   "source": [
    "Lets look at the original image and the bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fcb957-aaf2-4b6e-82a9-e6dd771b2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_and_bbox(image, bbox):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(tf.cast(image, tf.uint8))\n",
    "\n",
    "    # Create a Rectangle patch for the bounding box\n",
    "    rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], \n",
    "                             linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "visualize_image_and_bbox(image, bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2109bc-25e3-491f-934b-ee196d34a2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2557bb91-fd85-4663-a2f8-151ae0797440",
   "metadata": {},
   "source": [
    "# Translation\n",
    "Always ensure that the augmented bounding box remains within the image boundaries and maintains a valid size and aspect ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587a770a-1d42-40b0-9617-2fd210a0e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_image_and_bbox(image, bbox, dx, dy):\n",
    "    # Translate the image\n",
    "    translated_image = image.transform(\n",
    "        image.size, \n",
    "        Image.AFFINE, \n",
    "        (1, 0, dx, 0, 1, dy),\n",
    "        resample=Image.BICUBIC, fill=1)\n",
    "\n",
    "    # Adjust the bounding box\n",
    "    x_min, y_min, width, height = bbox\n",
    "    new_x_min = \n",
    "    new_y_min = \n",
    "    new_width = \n",
    "    new_height =\n",
    "    \n",
    "    translated_bbox = [new_x_min, new_y_min, new_width, new_height]\n",
    "    \n",
    "\n",
    "    return translated_image, translated_bbox\n",
    "\n",
    "dx, dy = 40, 30  # Translation values\n",
    "translated_image, translated_bbox = translate_image_and_bbox(image, bbox, dx, dy)\n",
    "visualize_image_and_bbox(translated_image, translated_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec989884-e516-4c44-be4b-d38bf9183284",
   "metadata": {},
   "source": [
    "# Mirroring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd4ad67-880a-450f-b7fa-f1e3ca883ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_image_and_bbox(image, bbox):\n",
    "    # Flip the image\n",
    "    flipped_image = image.transpose(method=Image.Transpose.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    # Adjust the bounding box\n",
    "    x_min, y_min, width, height = bbox\n",
    "\n",
    "    new_x_min=\n",
    "    new_y_min = \n",
    "    new_width = \n",
    "    new_height =\n",
    "    \n",
    "    flipped_bbox = [new_x_min, new_y_min, new_width, new_height]\n",
    "\n",
    "    return flipped_image, flipped_bbox\n",
    "\n",
    "flipped_box, flipped_bbox = flip_image_and_bbox(image, bbox)\n",
    "visualize_image_and_bbox(flipped_box, flipped_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4702ec-f16e-4e16-9f86-5452d6598165",
   "metadata": {},
   "source": [
    "# Zoom (scaling)\n",
    "To make this easier we're only zooming into the top left corner of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384729b-ba25-4e06-a292-3bd56f5ae4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zoom_scale_crop_image_and_bbox(image, bbox, zoom_factor, crop_size=(150, 150)):\n",
    "    original_width, original_height = image.size\n",
    "\n",
    "    # Calculate new dimensions\n",
    "    new_width = int(original_width * zoom_factor)\n",
    "    new_height = int(original_height * zoom_factor)\n",
    "\n",
    "    # Scale the image\n",
    "    scaled_image = image.resize((new_width, new_height), Image.BICUBIC)\n",
    "\n",
    "    # Adjust the bounding box\n",
    "    x_min, y_min, bbox_width, bbox_height = bbox\n",
    "    \n",
    "    new_x_min=\n",
    "    new_y_min = \n",
    "    new_width = \n",
    "    new_height =\n",
    "    \n",
    "    scaled_bbox = [new_x_min, new_y_min, new_width, new_height]\n",
    "\n",
    "    # Crop the scaled image\n",
    "    cropped_image = scaled_image.crop((0, 0, crop_size[0], crop_size[1]))\n",
    "\n",
    "    # Ensure the bounding box fits within the cropped dimensions\n",
    "    cropped_bbox = [\n",
    "        ,  # x_min\n",
    "        ,  # y_min\n",
    "        ,  # width\n",
    "           # height\n",
    "    ]\n",
    "\n",
    "    return cropped_image, cropped_bbox\n",
    "\n",
    "zoom_factor = 1.4 # Example zoom factor\n",
    "cropped_image, cropped_bbox = zoom_scale_crop_image_and_bbox(image, bbox, zoom_factor)\n",
    "visualize_image_and_bbox(cropped_image, cropped_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62c8260-2d30-4e3f-9fbe-72d53df36e76",
   "metadata": {},
   "source": [
    "# Rotation\n",
    "This is also a simplified function to keep this to an appropriate level.The rotation function rotates the image and then adjusts the position of the bounding box's center. **The dimensions of the bounding box remain the same.**\n",
    "\n",
    "The rotation angle is in degrees. Positive values rotate the image counterclockwise.\n",
    "This approach assumes that the rotation does not cause the bounding box to exceed the image boundaries. For larger angles or bounding boxes near the edge, additional logic is needed to handle these cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db256ee4-3564-4426-9812-57aebf0e81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_point(x, y, angle, ox, oy):\n",
    "    \"\"\"Rotate a point counterclockwise by a given angle around a given origin.\"\"\"\n",
    "    rad = np.radians(angle)\n",
    "    qx = ox + np.cos(rad) * (x - ox) - np.sin(rad) * (y - oy)\n",
    "    qy = oy + np.sin(rad) * (x - ox) + np.cos(rad) * (y - oy)\n",
    "    return qx, qy\n",
    "\n",
    "def rotate_image_and_bbox(image, bbox, angle):\n",
    "    # Center of rotation is the center of the image\n",
    "    ox, oy = image.width / 2, image.height / 2\n",
    "\n",
    "    # Coordinates of the bounding box corners\n",
    "    x_min, y_min, width, height = bbox\n",
    "    corners = [(x_min, y_min), (x_min + width, y_min), \n",
    "               (x_min, y_min + height), (x_min + width, y_min + height)]\n",
    "\n",
    "    # Rotate the image\n",
    "    rotated_image = image.rotate(-angle, expand=False)\n",
    "    \n",
    "    # Rotate the corners of the bounding box\n",
    "    rotated_corners = [rotate_point(x, y, angle, ox, oy) for (x, y) in corners]\n",
    "\n",
    "    # Determine new bounding box (min and max coordinates)\n",
    "    min_x = min([x for x, y in rotated_corners])\n",
    "    min_y = min([y for x, y in rotated_corners])\n",
    "    max_x = max([x for x, y in rotated_corners])\n",
    "    max_y = max([y for x, y in rotated_corners])\n",
    "\n",
    "    # New bounding box (x_min, y_min, width, height)\n",
    "    new_bbox = [min_x, min_y, max_x - min_x, max_y - min_y]\n",
    "\n",
    "    return rotated_image, new_bbox\n",
    "\n",
    "rotation_angle = 120  # Rotation angle in degrees\n",
    "rotated_image, new_bbox = rotate_image_and_bbox(image, bbox, rotation_angle)\n",
    "visualize_image_and_bbox(rotated_image, new_bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec31649f-e747-48a9-a9a2-5657ee19144f",
   "metadata": {},
   "source": [
    "# Generic augmentation\n",
    "Ok bounding boxes are hard to augment. Let's leave that for now. Instead now create, using the tensorflow preprocessing library, a function which does an random augmentation on an image given a range of allowed values. \n",
    "Play around with the ImageDataGenerator https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423099e-5b52-4e65-a66e-7a7fd6cf7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    # Fill in your parameters here\n",
    ")\n",
    "\n",
    "# Convert the image to a numpy array and add a batch dimension\n",
    "image_array = img_to_array(image)  # Convert to numpy array\n",
    "image_array = image_array[tf.newaxis, ...]  # Add the batch dimension\n",
    "\n",
    "# Apply the transformation.\n",
    "iterator = datagen.flow(image_array, batch_size=1)\n",
    "transformed_image = next(iterator)\n",
    "\n",
    "# plot the image\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.imshow(tf.cast(transformed_image[0], tf.uint8))  # Indexing at 0 to get the first image in the batch\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c45d78e-c215-4190-ba75-cef3781f720d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ccf753-3df7-4048-bf43-c1b689e7ecfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
