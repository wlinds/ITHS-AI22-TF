{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "449cff3e-32f8-464e-a101-ef7815ae6ab6",
   "metadata": {},
   "source": [
    "# Solving XOR using a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4285e4ba-86b3-4117-b3b4-4bb1ab21afce",
   "metadata": {},
   "source": [
    "XOR, or \"exclusive or\" is a simple function on 2 input values x_1 and x_2. \n",
    "\n",
    "\n",
    "![table](material/xor_table.png)\n",
    "\n",
    "It cannot be separated by a single decision boundary\n",
    "\n",
    "![graph](material/xor-graph.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953182be-abc7-4684-8457-76cf82df1ad1",
   "metadata": {},
   "source": [
    "We are going to make a network with 1 hidden layer of 2 neurons, approximating the XOR function. Since this is a binary classification problem we'll use the sigmoid activation function.\n",
    "\n",
    "![graph](material/xor_net.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ffa122-b84e-4405-8e5d-a53075b977c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdfc18a8-2a4b-4042-a28e-502e81a5dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random init weights and biases.\n",
    "VARIANCE = 0.5\n",
    "w11 = random.uniform(-VARIANCE,VARIANCE)\n",
    "w21 = random.uniform(-VARIANCE,VARIANCE)\n",
    "b1 = random.uniform(-VARIANCE,VARIANCE)\n",
    "\n",
    "w12 = random.uniform(-VARIANCE,VARIANCE)\n",
    "w22 = random.uniform(-VARIANCE,VARIANCE)\n",
    "b2 = random.uniform(-VARIANCE,VARIANCE)\n",
    "\n",
    "o1 = random.uniform(-VARIANCE,VARIANCE)\n",
    "o2 = random.uniform(-VARIANCE,VARIANCE)\n",
    "ob = random.uniform(-VARIANCE,VARIANCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea933773-7dc0-4d04-ae83-e6d0fcc25da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our activation function for hidden units\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + math.exp(-x))\n",
    "\n",
    "# Derivative of sigmoid function.\n",
    "def dSigmoid(x): # x already sigmoided from layer activation.\n",
    "    return x * (1 - x)\n",
    "\n",
    "\n",
    "def predict(i1,i2):    \n",
    "    h1 = w11 * i1 + w21 * i2 + b1\n",
    "    h1 = sigmoid(h1)\n",
    "    h2 = w12 * i1 + w22 * i2 + b2\n",
    "    h2 = sigmoid(h2)\n",
    "\n",
    "    output = h1 * o1 + h2 * o2 + ob\n",
    "    output = sigmoid(output)\n",
    "    \n",
    "    return output, h1, h2\n",
    "\n",
    "def learn(i1,i2,target, lr=0.2):\n",
    "    global w11,w21,b1,w12,w22,b2\n",
    "    global o1,o2,ob\n",
    "\n",
    "    # ---- Forward pass\n",
    "    output, h1, h2 = predict(i1,i2)\n",
    "    \n",
    "    # --- Calculate error and our derivate\n",
    "    error = (output - target) # MAE\n",
    "    derror = error * dSigmoid(output)\n",
    "\n",
    "    # Update output weights and bias.\n",
    "    o1 -= lr * h1 * derror\n",
    "    o2 -= lr * h2 * derror\n",
    "    ob -= lr * derror\n",
    "\n",
    "    # Get the gradient for the hidden layers.\n",
    "    dh1 = derror * o1 * dSigmoid(h1)\n",
    "    dh2 = derror * o2 * dSigmoid(h2)\n",
    "\n",
    "    # Update hidden layer weights and bias.\n",
    "    w11 -= lr * i1 * dh1\n",
    "    w21 -= lr * i2 * dh1\n",
    "    b1 -= lr * dh1\n",
    "    w12 -= lr * i1 * dh2\n",
    "    w22 -= lr * i2 * dh2\n",
    "    b2 -= lr * dh2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ab2e1e8-900f-45b2-af53-ee7777318d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1000 mean squared error: 0.2468447981619645\n",
      "epoch 2000 mean squared error: 0.1842382808292103\n",
      "epoch 3000 mean squared error: 0.043978130508925406\n",
      "epoch 4000 mean squared error: 0.008117046539117772\n",
      "epoch 5000 mean squared error: 0.004032874064066842\n",
      "epoch 6000 mean squared error: 0.002621656614528803\n",
      "epoch 7000 mean squared error: 0.0019235943290886598\n",
      "epoch 8000 mean squared error: 0.0015114844718468285\n",
      "epoch 9000 mean squared error: 0.0012410362682904554\n",
      "epoch 10000 mean squared error: 0.001050589739319582\n",
      "for input [0, 0] expected 0 predicted 0.03485 which is correct\n",
      "for input [0, 1] expected 1 predicted 0.969 which is correct\n",
      "for input [1, 0] expected 1 predicted 0.9691 which is correct\n",
      "for input [1, 1] expected 0 predicted 0.03273 which is correct\n"
     ]
    }
   ],
   "source": [
    "INPUTS = [\n",
    "        [0,0],\n",
    "        [0,1],\n",
    "        [1,0],\n",
    "        [1,1]\n",
    "    ]\n",
    "\n",
    "OUTPUTS = [\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0]\n",
    "    ]\n",
    "\n",
    "\n",
    "for epoch in range(1,10001):\n",
    "    indexes = [0,1,2,3]\n",
    "    random.shuffle(indexes)\n",
    "    for j in indexes:\n",
    "        learn(INPUTS[j][0],INPUTS[j][1],OUTPUTS[j][0], lr=0.2)\n",
    "    \n",
    "    if epoch%1000 == 0:\n",
    "        cost = 0\n",
    "        for j in range(4):\n",
    "            o, _, _= predict(INPUTS[j][0],INPUTS[j][1])\n",
    "            cost += (OUTPUTS[j][0] - o) ** 2\n",
    "        cost /= 4\n",
    "        print(\"epoch\", epoch, \"mean squared error:\", cost)       \n",
    "        \n",
    "\n",
    "for i in range(4):\n",
    "    result, _, _= predict(INPUTS[i][0],INPUTS[i][1])\n",
    "    print(\"for input\", INPUTS[i], \"expected\", OUTPUTS[i][0], \"predicted\", f\"{result:4.4}\", \"which is\", \"correct\" if round(result)==OUTPUTS[i][0] else \"incorrect\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141d6bdb-850d-42ed-a1c6-f75ede0f1eb0",
   "metadata": {},
   "source": [
    "It does not always learn perfectly since this is a very crude network with a fixed learning rate and fixed nr epochs. But if you run it a couple times it should converge into a network which correctly classifies all possible xor inputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
