{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Data/hate-text.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial model test without handling negation and polarity. We simply trust the algo to pick up on the patterns of negations. Which probably **won't** be successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base keras model\n",
    "\n",
    "No cleaning, no regularization, no nothin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Preprocess\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['test_case'])\n",
    "\n",
    "X = tokenizer.texts_to_sequences(df['test_case'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "y = (df['label_gold'] == 'hateful').astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=X.shape[1]))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "78/78 [==============================] - 1s 4ms/step - loss: 0.6023 - accuracy: 0.6867 - val_loss: 0.5551 - val_accuracy: 0.7276\n",
      "Epoch 2/5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.4662 - accuracy: 0.7865 - val_loss: 0.4237 - val_accuracy: 0.8061\n",
      "Epoch 3/5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.9002 - val_loss: 0.3225 - val_accuracy: 0.8814\n",
      "Epoch 4/5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.1970 - accuracy: 0.9495 - val_loss: 0.2676 - val_accuracy: 0.9103\n",
      "Epoch 5/5\n",
      "78/78 [==============================] - 0s 3ms/step - loss: 0.1356 - accuracy: 0.9683 - val_loss: 0.2368 - val_accuracy: 0.9199\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1818 - accuracy: 0.9437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1818477213382721, 0.9436619877815247)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "Text: 'I hate women.'\n",
      "Predicted Label: Hateful\n",
      "Prediction Score: 0.7363396883010864\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Text: 'I don't hate women.'\n",
      "Predicted Label: Hateful\n",
      "Prediction Score: 0.5670516490936279\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Text: 'I don't love women'\n",
      "Predicted Label: Not Hateful\n",
      "Prediction Score: 0.44292059540748596\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Text: 'I love women'\n",
      "Predicted Label: Hateful\n",
      "Prediction Score: 0.6289886236190796\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text, tokenizer):\n",
    "    sequences = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=X.shape[1])\n",
    "    return padded_sequences\n",
    "\n",
    "def predict_hatefulness(model, text, tokenizer):\n",
    "    processed_text = preprocess_text(text, tokenizer)\n",
    "    prediction = model.predict(processed_text)[0, 0]\n",
    "    \n",
    "    label = 'Hateful' if prediction >= 0.5 else 'Not Hateful'\n",
    "    \n",
    "    return label, prediction\n",
    "\n",
    "new_text = [\"I hate women.\", \"I don't hate women.\", \"I don't love women\", \"I love women\"]\n",
    "\n",
    "for i in new_text:\n",
    "    label, prediction = predict_hatefulness(model, i, tokenizer)\n",
    "    print(f\"Text: '{i}'\")\n",
    "    print(f\"Predicted Label: {label}\")\n",
    "    print(f\"Prediction Score: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model is unable to pick up negations from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative terms include no, not, won't, shouldn’t, etc. When a negation appears in a sentence, it is critical to determine which words are impacted by this phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negation terms like these are used to perform sentiment analysis of a sentence, a phrase or even a paragraph. To process these words, we define what is called a Sentence Polarity.\n",
    "\n",
    "The sentence polarity is calculated on the basis of the parts of a sentence. A sentence may contain either simple POS (Verb, Adverb, Adjectives, etc.) or complex parts of\n",
    "speech (Noun Phrase [Pronoun, Noun] or Verb Phrase [Verb, Noun Phrase], relations of possession, determiner, etc.). The following hierarchy is an example of POS in a complete sentence.\n",
    "\n",
    "(Sentence\n",
    "(Noun Phrase (Pronoun, Noun))\n",
    "(Adverbial Phrase (Adverb))\n",
    "(Verb Phrase (Verb)\n",
    "(Sentence\n",
    "(Verb Phrase (Verb)\n",
    "(Noun Phrase (Noun))\n",
    ") ) ) )\n",
    "\n",
    "\n",
    "Sentiment polarity calculation is a nested process. This process calculates the sentiment of the most inner level first and then it calculates along with the next higher level, which is also called Sentiment Propagation. This process calculates the polarity and intensity of the words and phrases. If there is a negation term, the polarity will be calculated accordingly. The following three examples illustrate the whole process of polarity calculation.\n",
    "\n",
    "A. Example 1::\n",
    "They have not succeeded, and will never succeed, in\n",
    "breaking the will of this valiant people.\n",
    "(Sentence\n",
    "(Pronoun They)\n",
    "(Verb Phrase\n",
    "(Verb Phrase (have not)\n",
    "(Verb Phrase (Verb succeeded)))\n",
    "(and)\n",
    "(Verb Phrase (will)\n",
    "(Adverbial Phrase (Adverb never))\n",
    "(Verb Phrase (succeed)))\n",
    "(Prepositional Phrase (in)\n",
    "(Sentence\n",
    "(Verb Phrase (breaking)\n",
    "(Noun Phrase\n",
    "(Noun Phrase (the will))\n",
    "(Prepositional Phrase (of)\n",
    "(Noun Phrase (this valiant people)))))))))\n",
    "\n",
    "The negation word ‘not’ is affecting the succeeded (+) whereas never is effecting succeed (+) where succeeded and succeed are joined by 'and' (joins same polarity). Both successes are in breaking (-) the will of people who are valiant (+) people. As they have not succeeded in doing something 'Negative' and the polarity of sentence is 'Positive'.\n",
    "\n",
    "Source: [Ashudeep Singh, Quora](https://www.quora.com/NLP-whats-the-best-method-to-detect-negated-contexts-in-text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible solution\n",
    "\n",
    "All negation words are divided into three categories.\n",
    "\n",
    "- All negations that totally reverse the polarity of other words are classified as syntactic negations.\n",
    "- The diminisher class covers all negation words that lessen the polarities rather than inverting them.\n",
    "- All prefixes and suffixes that can be used to produce a morphological negative are included in the morphological class. These prefixes and suffixes are also employed to identify the existence of a morphological negative.\n",
    "\n",
    "[source](https://analyticsindiamag.com/when-to-use-negation-handling-in-sentiment-analysis/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with pre-trained\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/<br>\n",
    "https://spacy.io/universe/project/spacy-stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 20:05:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6118331698fe4da5859c37cb13fa9b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 20:05:53 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2024-01-22 20:05:53 WARNING: GPU requested, but is not available!\n",
      "2024-01-22 20:05:53 INFO: Using device: cpu\n",
      "2024-01-22 20:05:53 INFO: Loading: tokenize\n",
      "2024-01-22 20:05:53 INFO: Loading: pos\n",
      "2024-01-22 20:05:54 INFO: Loading: lemma\n",
      "2024-01-22 20:05:54 INFO: Loading: constituency\n",
      "2024-01-22 20:05:55 INFO: Loading: depparse\n",
      "2024-01-22 20:05:55 INFO: Loading: sentiment\n",
      "2024-01-22 20:05:55 INFO: Loading: ner\n",
      "2024-01-22 20:05:56 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "import stanza \n",
    "import spacy_stanza\n",
    "from negspacy.negation import Negex\n",
    "from negspacy.termsets import termset \n",
    "\n",
    "nlp_model = spacy_stanza.load_pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<negspacy.negation.Negex at 0x14e070dd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_model.add_pipe(\"negex\", config={\"ent_types\":[\"PERSON\",\"ORG\",\"CARDINAL\", \"DATE\", \"EVENT\", \"LANGUAGE\", \"PRODUCT\", \"QUANTITY\", \"TIME\", \"WORK_OF_ART\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English True\n",
      "Adolf Hitler True\n",
      "German False\n"
     ]
    }
   ],
   "source": [
    "sample = nlp_model('There is no English language option.')\n",
    " \n",
    "for e in sample.ents:\n",
    "  print(e.text, e._.negex)\n",
    "\n",
    "doc = nlp_model('He does not like Adolf Hitler but likes German products.')\n",
    " \n",
    "for e in doc.ents:\n",
    "  print(e.text, e._.negex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true indicates the word has a negative meaning and the false indicates the positive sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21780acb06b645549cdb7929cc9ccbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 20:32:13 INFO: Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0928644412b248dcb3ffe3bedd1c95ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.6.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 20:32:42 INFO: Finished downloading models and saved to /Users/helvetica/stanza_resources.\n",
      "2024-01-22 20:32:42 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d190ae391947a0ae194ea918fbdf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 20:32:44 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2024-01-22 20:32:44 INFO: Using device: cpu\n",
      "2024-01-22 20:32:44 INFO: Loading: tokenize\n",
      "2024-01-22 20:32:44 INFO: Loading: pos\n",
      "2024-01-22 20:32:44 INFO: Loading: lemma\n",
      "2024-01-22 20:32:44 INFO: Loading: constituency\n",
      "2024-01-22 20:32:45 INFO: Loading: depparse\n",
      "2024-01-22 20:32:46 INFO: Loading: sentiment\n",
      "2024-01-22 20:32:46 INFO: Loading: ner\n",
      "2024-01-22 20:32:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', use_gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"Immigrants\",\n",
      "      \"lemma\": \"immigrant\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NNS\",\n",
      "      \"feats\": \"Number=Plur\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 10,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"like\",\n",
      "      \"lemma\": \"like\",\n",
      "      \"upos\": \"ADP\",\n",
      "      \"xpos\": \"IN\",\n",
      "      \"head\": 3,\n",
      "      \"deprel\": \"case\",\n",
      "      \"start_char\": 11,\n",
      "      \"end_char\": 15,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"you\",\n",
      "      \"lemma\": \"you\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"PRP\",\n",
      "      \"feats\": \"Case=Acc|Person=2|PronType=Prs\",\n",
      "      \"head\": 1,\n",
      "      \"deprel\": \"nmod\",\n",
      "      \"start_char\": 16,\n",
      "      \"end_char\": 19,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"do\",\n",
      "      \"lemma\": \"do\",\n",
      "      \"upos\": \"AUX\",\n",
      "      \"xpos\": \"VBP\",\n",
      "      \"feats\": \"Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"aux\",\n",
      "      \"start_char\": 20,\n",
      "      \"end_char\": 22,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"not\",\n",
      "      \"lemma\": \"not\",\n",
      "      \"upos\": \"PART\",\n",
      "      \"xpos\": \"RB\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"advmod\",\n",
      "      \"start_char\": 23,\n",
      "      \"end_char\": 26,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \"deserve\",\n",
      "      \"lemma\": \"deserve\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VB\",\n",
      "      \"feats\": \"VerbForm=Inf\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 27,\n",
      "      \"end_char\": 34,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 7,\n",
      "      \"text\": \"to\",\n",
      "      \"lemma\": \"to\",\n",
      "      \"upos\": \"PART\",\n",
      "      \"xpos\": \"TO\",\n",
      "      \"head\": 8,\n",
      "      \"deprel\": \"mark\",\n",
      "      \"start_char\": 35,\n",
      "      \"end_char\": 37,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 8,\n",
      "      \"text\": \"live\",\n",
      "      \"lemma\": \"live\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VB\",\n",
      "      \"feats\": \"VerbForm=Inf\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"xcomp\",\n",
      "      \"start_char\": 38,\n",
      "      \"end_char\": 42,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"id\": 9,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 6,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 42,\n",
      "      \"end_char\": 43,\n",
      "      \"ner\": \"O\",\n",
      "      \"multi_ner\": [\n",
      "        \"O\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Immigrants like you do not deserve to live.\")\n",
    "\n",
    "print(doc)\n",
    "print(doc.entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigrants immigrant NOUN\n",
      "like like ADP\n",
      "you you PRON\n",
      "do do AUX\n",
      "not not PART\n",
      "deserve deserve VERB\n",
      "to to PART\n",
      "live live VERB\n",
      ". . PUNCT\n"
     ]
    }
   ],
   "source": [
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(word.text, word.lemma, word.pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c0061f4a2a44acbebbba11de6b8314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 20:41:30 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-01-22 20:41:32 INFO: File exists: /Users/helvetica/stanza_resources/en/default.zip\n",
      "2024-01-22 20:41:37 INFO: Finished downloading models and saved to /Users/helvetica/stanza_resources.\n",
      "2024-01-22 20:41:37 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e34341bd0f34c7c84b3b051ef795e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 20:41:40 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2024-01-22 20:41:40 INFO: Using device: cpu\n",
      "2024-01-22 20:41:40 INFO: Loading: tokenize\n",
      "2024-01-22 20:41:40 INFO: Loading: pos\n",
      "2024-01-22 20:41:40 INFO: Loading: lemma\n",
      "2024-01-22 20:41:40 INFO: Loading: constituency\n",
      "2024-01-22 20:41:41 INFO: Loading: depparse\n",
      "2024-01-22 20:41:41 INFO: Loading: sentiment\n",
      "2024-01-22 20:41:42 INFO: Loading: ner\n",
      "2024-01-22 20:41:42 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "stanza.download('en')\n",
    "nlp = stanza.Pipeline('en', use_gpu=False)\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    return tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
