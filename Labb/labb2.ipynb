{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN/GRU Model: Text Classification & Sentiment Analysis\n",
    "\n",
    "This project implements a neural network for classifying emotions in text. The dataset used for training and evaluation is a 20 000 line csv-file, each line containing a string along with one of six labels: 'anger', 'sadness', 'joy', 'love', 'fear' or 'surprise'.\n",
    "\n",
    "A custom model has been created from the ground up with Keras. The data has been appropriately split into training, validation, and test #TODO sets. Activation functions and loss functions were selected to optimize performance.\n",
    "\n",
    "**The project includes comprehensive prints of relevant metrics and visualizations of training curves. #TODO viz**\n",
    "\n",
    "---\n",
    "\n",
    "- *Data Preprocessing*: Removing unnecessary elements.\n",
    "- *Model Architecture*:\n",
    "   - *Activation Functions*: Softmax.\n",
    "   - *Loss Function*: Categorical Crossentropy.\n",
    "   - *Regularization Techniques*: Dropout in GRU-layer.\n",
    "- *Evaluation*: n/a #TODO\n",
    "\n",
    "- **Reflection and Future Improvements**:\n",
    "   - The project reflects on the achieved performance and discusses potential enhancements.\n",
    "   - Suggestions for further improvement are outlined, showcasing a critical evaluation of the project.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The initial EDA provides insights into the dataset structure:\n",
    "\n",
    "```plaintext\n",
    "   text                                                    label\n",
    "   count   20000                                           20000\n",
    "   unique  19948                                           6\n",
    "   top     i tend to stop breathing when i m feeling s...  joy\n",
    "   freq    2                                               6761\n",
    "```\n",
    "\n",
    "The dataset comprises 20,000 entries, with 19,948 unique texts and six unique emotion labels. The most frequent emotion is 'joy' with 6,761 occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>19948</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>i tend to stop breathing when i m feeling stre...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>6761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "count                                               20000  20000\n",
       "unique                                              19948      6\n",
       "top     i tend to stop breathing when i m feeling stre...    joy\n",
       "freq                                                    2   6761"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data/emotions.csv')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "joy         6761\n",
       "sadness     5797\n",
       "anger       2709\n",
       "fear        2373\n",
       "love        1641\n",
       "surprise     719\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class distribution exhibits varying degrees of skewness, with 'joy' and 'Sadness' being the most prevalent emotions in the dataset. Adjustments may be considered to ensure the model achieves a balanced and accurate representation across all emotional categories. #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>i feel like all she wants is his parents fortu...</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4386</th>\n",
       "      <td>i just think about all the day i chatted with ...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16451</th>\n",
       "      <td>i miss everybody i am still feeling relieved b...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14320</th>\n",
       "      <td>i wasnt exactly sure how i was going to feel a...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13768</th>\n",
       "      <td>i feel now i am not giving all of me to christ...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9067</th>\n",
       "      <td>i feel like im not as stubborn</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7932</th>\n",
       "      <td>im left feeling convinced this is another rela...</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10233</th>\n",
       "      <td>i feel fucked</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5230</th>\n",
       "      <td>im feeling like a tortured teen i decided to p...</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2344</th>\n",
       "      <td>i did not enjoy the feeling of the naughty kid...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text    label\n",
       "3224   i feel like all she wants is his parents fortu...  sadness\n",
       "4386   i just think about all the day i chatted with ...     love\n",
       "16451  i miss everybody i am still feeling relieved b...      joy\n",
       "14320  i wasnt exactly sure how i was going to feel a...      joy\n",
       "13768  i feel now i am not giving all of me to christ...     love\n",
       "9067                      i feel like im not as stubborn    anger\n",
       "7932   im left feeling convinced this is another rela...      joy\n",
       "10233                                      i feel fucked    anger\n",
       "5230   im feeling like a tortured teen i decided to p...     fear\n",
       "2344   i did not enjoy the feeling of the naughty kid...     love"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "First iteration cleaning by just running `df.sample()` a couple of times and picking out irrelevant words.\n",
    "\n",
    "The following URL-related elements and common words are identified for removal:\n",
    "\n",
    "```\n",
    "url_elements = ['http', 'href', 'www', 'src']\n",
    "common_words = ['i', 'id', 'im', 'ive', 've', 'is', 'to', 'am', 'feel', 'feeling', 'your']\n",
    "```\n",
    "\n",
    "The `remove_words()` function is then applied to create a new 'text_clean' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'href' occurs 207 times in the column 'text'.\n",
      "It occurs in the following rows: [3, 19, 88, 91, 200, 267, 351, 425, 560, 623, 731, 824, 959, 1103, 1314, 1372, 1402, 1543, 1657, 1700, 1737, 1803, 1873, 2024, 2026, 2213, 2225, 2253, 2623, 2969, 3016, 3478, 3525, 3568, 3631, 3823, 3832, 3882, 4071, 4079, 4138, 4142, 4144, 4335, 4365, 4405, 4667, 4690, 4722, 5009, 5011, 5117, 5438, 5596, 5634, 5684, 5757, 5964, 6023, 6050, 6448, 6513, 6761, 6765, 6821, 6913, 7003, 7216, 7320, 7325, 7401, 7597, 7876, 7877, 8112, 8341, 8652, 8675, 8872, 8880, 8956, 9084, 9161, 9320, 9359, 9363, 9454, 9480, 9487, 9488, 9638, 9724, 10124, 10228, 10327, 10403, 10456, 10506, 10564, 10625, 10718, 10959, 11003, 11024, 11082, 11304, 11306, 11360, 11368, 11409, 11460, 11587, 11600, 11603, 11686, 11734, 11792, 11978, 12102, 12235, 12298, 12352, 12530, 12545, 12617, 12633, 12693, 12976, 13006, 13019, 13154, 13252, 13274, 13295, 13442, 13466, 13581, 13594, 13609, 13755, 13757, 13791, 13793, 13832, 13918, 13922, 14083, 14326, 14440, 14484, 14499, 14591, 14613, 14724, 14764, 14794, 14803, 14946, 14977, 15189, 15522, 15661, 15764, 15799, 15977, 16002, 16067, 16090, 16097, 16099, 16158, 16406, 16491, 16500, 16593, 16597, 16600, 16665, 16780, 16812, 17043, 17150, 17195, 17416, 17661, 17848, 17937, 17942, 18175, 18204, 18213, 18467, 18516, 18610, 18734, 18758, 18808, 18905, 18939, 19101, 19304, 19397, 19454, 19591, 19791, 19817, 19871]\n"
     ]
    }
   ],
   "source": [
    "def get_freq(column, search_term, return_rows=False, verbose=False):\n",
    "    n, rows = 0, []\n",
    "    for row, text in column.items():\n",
    "        if search_term in str(text):\n",
    "            n += 1\n",
    "            rows.append(row)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"The word '{search_term}' occurs {n} times in the column '{column.name}'.\")\n",
    "        if n > 0:\n",
    "            print(f\"It occurs in the following rows: {rows}\")\n",
    "\n",
    "    if return_rows:\n",
    "        return n, rows\n",
    "    else:\n",
    "        return n\n",
    "\n",
    "search_term = \"href\"\n",
    "freq, rows = get_freq(df['text'], search_term, return_rows=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_elements = ['http', 'href', 'www', 'src']\n",
    "common_words = ['i', 'id', 'im', 'ive', 've', 'is', 'to', 'am', 'feel', 'feeling', 'your']\n",
    "\n",
    "def remove_words(text_string, stop_words):\n",
    "    words = text_string.split()\n",
    "    filtered = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered)\n",
    "\n",
    "df['text_clean'] = df['text'].apply(lambda x: remove_words(x, common_words + url_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          i feel petty a href http clairee\n",
       "label                                    anger\n",
       "text_clean                     petty a clairee\n",
       "Name: 3, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me too, I also feel petty a href http clairee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'sadness', 'joy', 'love', 'fear', 'surprise'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Analysis\n",
    "An analysis of word frequency identifies words that occur only once (8,668 words). These words are considered for removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger occurs 155 times.\n",
      "sadness occurs 35 times.\n",
      "joy occurs 296 times.\n",
      "love occurs 929 times.\n",
      "fear occurs 152 times.\n",
      "surprise occurs 102 times.\n"
     ]
    }
   ],
   "source": [
    "label_list = df['label'].unique().tolist()\n",
    "for i in label_list:\n",
    "    freq = get_freq(df['text'], i)\n",
    "    print(f\"{i} occurs {freq} times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's that sentiment?\n",
      "i am also posting this because i am trying to work on the writing i want my students to feel passionate about\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "# Hol up, it's mini game time\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) \n",
    "\n",
    "def guess_the_label(df=df):\n",
    "    sample = df.sample()\n",
    "    print(\"What's that sentiment?\")\n",
    "    print((sample['text'].iloc[0]))\n",
    "    a = input('Enter your guesa')\n",
    "    if str(a) == str(sample['label'].iloc[0]):\n",
    "        print(\"Correct!\")\n",
    "    else:\n",
    "        print(f\"Too bad! It was {sample['label'].iloc[0]}\")\n",
    "\n",
    "guess_the_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_texts = df['text_clean'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 11996),\n",
       " ('the', 10462),\n",
       " ('a', 7748),\n",
       " ('that', 6314),\n",
       " ('of', 6182),\n",
       " ('my', 5326),\n",
       " ('in', 4239),\n",
       " ('it', 3922),\n",
       " ('like', 3616),\n",
       " ('so', 3127)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_words(corpus_texts=corpus_texts, n=10, least = False):\n",
    "    all_text = ' '.join(corpus_texts)\n",
    "    words = all_text.split()\n",
    "    word_counts = Counter(words)\n",
    "\n",
    "    if n == None:\n",
    "        return word_counts\n",
    "        \n",
    "    if least:\n",
    "        return word_counts.most_common()[-n-1:-1]\n",
    "    \n",
    "    return word_counts.most_common(n)\n",
    "\n",
    "get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'the', 'a', 'that', 'of', 'my', 'in', 'it', 'like', 'so']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_stop_words = [w for w, c in get_words()]\n",
    "new_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_common = get_words(n=20, least=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('murmur', 1),\n",
       " ('chastised', 1),\n",
       " ('rubble', 1),\n",
       " ('everyfuckingthing', 1),\n",
       " ('passive', 1),\n",
       " ('dissected', 1),\n",
       " ('biomedical', 1),\n",
       " ('mombasa', 1),\n",
       " ('digg', 1),\n",
       " ('unconcern', 1),\n",
       " ('hovel', 1),\n",
       " ('riots', 1),\n",
       " ('pipsqueak', 1),\n",
       " ('condemn', 1),\n",
       " ('dipped', 1),\n",
       " ('everchanging', 1),\n",
       " ('sailormoon', 1),\n",
       " ('gateway', 1),\n",
       " ('lathi', 1),\n",
       " ('sonam', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_common"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These all seem pretty irrelevant. But we can probably remove even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 8668),\n",
       " (2, 2457),\n",
       " (3, 1202),\n",
       " (4, 766),\n",
       " (5, 500),\n",
       " (6, 376),\n",
       " (7, 289),\n",
       " (8, 218),\n",
       " (9, 179),\n",
       " (10, 142)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(get_words(corpus_texts, n=None).values()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [word for word, count in  get_words(df['text_clean'], n=None).items() if count == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8668"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_clean'] = df['text_clean'].apply(lambda x:remove_words(x, stop_words+new_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17810</th>\n",
       "      <td>i feel like i shouldnt bother people with these petty stupid little pathetic thoughts i feel like no one really would care to know what really goes on inside my head</td>\n",
       "      <td>anger</td>\n",
       "      <td>shouldnt bother people with these petty stupid little pathetic thoughts no one really would care know what really goes on inside head</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7973</th>\n",
       "      <td>i felt like facebook was a catalyst for me to feel that way about myself and i started to see it as a bit of a hostile online community</td>\n",
       "      <td>anger</td>\n",
       "      <td>felt facebook was for me way about myself started see as bit hostile online community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>i feel sure the nervousness and fear will always lurk in my mind but i feel at ease in my heart hopeful about theo ad and eli being happy healthy and safe and living to be old people with fulfilled lives</td>\n",
       "      <td>joy</td>\n",
       "      <td>sure nervousness fear will always mind but at ease heart hopeful about ad being happy healthy safe living be old people with fulfilled lives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7809</th>\n",
       "      <td>i feel edmontonians are superior to the residents of any other major city but if you ask me what keeps me living here despite my obvious hatred for the climate of the year then my response is family and friends</td>\n",
       "      <td>joy</td>\n",
       "      <td>are superior residents any other major city but if you ask me what keeps me living here despite obvious hatred for climate year then response family friends</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15872</th>\n",
       "      <td>i feel liked these days by both tom and myself</td>\n",
       "      <td>love</td>\n",
       "      <td>liked these days by both tom myself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2397</th>\n",
       "      <td>i ended the podcast feeling not depressed exactly but like i still didn t have a concrete answer for how to strike that balance that self help authors love to talk about</td>\n",
       "      <td>sadness</td>\n",
       "      <td>ended podcast not depressed exactly but still didn t have answer for how strike balance self help authors love talk about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18804</th>\n",
       "      <td>i am living with my dad and his wife in his new home and i feel very unwelcome here</td>\n",
       "      <td>sadness</td>\n",
       "      <td>living with dad his wife his new home very unwelcome here</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3194</th>\n",
       "      <td>i want to come out about it but i feel so reluctant for some reason</td>\n",
       "      <td>fear</td>\n",
       "      <td>want come out about but reluctant for some reason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15574</th>\n",
       "      <td>im feeling rather inspired yet low i will enjoy my writing and even though i may be writing about morbid things i will find a way to make it interesting to read</td>\n",
       "      <td>joy</td>\n",
       "      <td>rather inspired yet low will enjoy writing even though may be writing about morbid things will find way make interesting read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15104</th>\n",
       "      <td>i would be feeling guilty of writing craps on my blog nothing useful nor beneficial to others</td>\n",
       "      <td>sadness</td>\n",
       "      <td>would be guilty writing on blog nothing useful nor beneficial others</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                     text  \\\n",
       "17810                                               i feel like i shouldnt bother people with these petty stupid little pathetic thoughts i feel like no one really would care to know what really goes on inside my head   \n",
       "7973                                                                              i felt like facebook was a catalyst for me to feel that way about myself and i started to see it as a bit of a hostile online community   \n",
       "1518          i feel sure the nervousness and fear will always lurk in my mind but i feel at ease in my heart hopeful about theo ad and eli being happy healthy and safe and living to be old people with fulfilled lives   \n",
       "7809   i feel edmontonians are superior to the residents of any other major city but if you ask me what keeps me living here despite my obvious hatred for the climate of the year then my response is family and friends   \n",
       "15872                                                                                                                                                                      i feel liked these days by both tom and myself   \n",
       "2397                                            i ended the podcast feeling not depressed exactly but like i still didn t have a concrete answer for how to strike that balance that self help authors love to talk about   \n",
       "18804                                                                                                                                 i am living with my dad and his wife in his new home and i feel very unwelcome here   \n",
       "3194                                                                                                                                                  i want to come out about it but i feel so reluctant for some reason   \n",
       "15574                                                    im feeling rather inspired yet low i will enjoy my writing and even though i may be writing about morbid things i will find a way to make it interesting to read   \n",
       "15104                                                                                                                       i would be feeling guilty of writing craps on my blog nothing useful nor beneficial to others   \n",
       "\n",
       "         label  \\\n",
       "17810    anger   \n",
       "7973     anger   \n",
       "1518       joy   \n",
       "7809       joy   \n",
       "15872     love   \n",
       "2397   sadness   \n",
       "18804  sadness   \n",
       "3194      fear   \n",
       "15574      joy   \n",
       "15104  sadness   \n",
       "\n",
       "                                                                                                                                                         text_clean  \n",
       "17810                         shouldnt bother people with these petty stupid little pathetic thoughts no one really would care know what really goes on inside head  \n",
       "7973                                                                          felt facebook was for me way about myself started see as bit hostile online community  \n",
       "1518                   sure nervousness fear will always mind but at ease heart hopeful about ad being happy healthy safe living be old people with fulfilled lives  \n",
       "7809   are superior residents any other major city but if you ask me what keeps me living here despite obvious hatred for climate year then response family friends  \n",
       "15872                                                                                                                           liked these days by both tom myself  \n",
       "2397                                      ended podcast not depressed exactly but still didn t have answer for how strike balance self help authors love talk about  \n",
       "18804                                                                                                     living with dad his wife his new home very unwelcome here  \n",
       "3194                                                                                                              want come out about but reluctant for some reason  \n",
       "15574                                 rather inspired yet low will enjoy writing even though may be writing about morbid things will find way make interesting read  \n",
       "15104                                                                                          would be guilty writing on blog nothing useful nor beneficial others  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'sadness', 'joy', 'love', 'fear', 'surprise'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14941"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text_clean'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'more important than going fun ipad strategy games original boots from ugg wear ugg boots this winter low cost ugg boots uggs need get washed inside they are also you won t burdened with them speed up finances with payday loans payday loans monthly solution for you'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df['text_clean'], key=len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whAT, uh anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for the Model\n",
    "\n",
    "The next step involves preparing the data for the model. This includes label encoding and one-hot encoding for the target variable.\n",
    "\n",
    "### One hot encoding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'sadness', 'joy', 'love', 'fear', 'surprise'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "integer_labels = label_encoder.fit_transform(df['label'])\n",
    "one_hot_labels = to_categorical(integer_labels)\n",
    "\n",
    "assert len(one_hot_labels) == 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "The text data is tokenized using the Keras Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8404"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text_clean'])\n",
    "\n",
    "len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('pissed', 77),\n",
       "             ('off', 277),\n",
       "             ('over', 426),\n",
       "             ('an', 700),\n",
       "             ('old', 148),\n",
       "             ('friend', 147),\n",
       "             ('some', 750),\n",
       "             ('friends', 253),\n",
       "             ('found', 123),\n",
       "             ('has', 500),\n",
       "             ('made', 368),\n",
       "             ('huge', 36),\n",
       "             ('difference', 31),\n",
       "             ('especially', 148),\n",
       "             ('on', 1925),\n",
       "             ('finger', 15),\n",
       "             ('with', 2530),\n",
       "             ('ring', 8),\n",
       "             ('skin', 59),\n",
       "             ('feels', 416),\n",
       "             ('much', 618),\n",
       "             ('less', 308),\n",
       "             ('irritated', 79),\n",
       "             ('also', 423),\n",
       "             ('unfortunate', 55),\n",
       "             ('nearly', 27),\n",
       "             ('all', 1456),\n",
       "             ('readers', 39),\n",
       "             ('going', 606),\n",
       "             ('meet', 53),\n",
       "             ('man', 107),\n",
       "             ('will', 887),\n",
       "             ('be', 2193),\n",
       "             ('african', 7),\n",
       "             ('americans', 8),\n",
       "             ('unlike', 9),\n",
       "             ('myself', 1004),\n",
       "             ('petty', 64),\n",
       "             ('used', 122),\n",
       "             ('believe', 134),\n",
       "             ('fear', 55),\n",
       "             ('was', 2828),\n",
       "             ('ignored', 68),\n",
       "             ('or', 1564),\n",
       "             ('suppressed', 5),\n",
       "             ('right', 444),\n",
       "             ('away', 276),\n",
       "             ('more', 1396),\n",
       "             ('this', 2606),\n",
       "             ('moment', 168),\n",
       "             ('sit', 80),\n",
       "             ('same', 192),\n",
       "             ('hostel', 5),\n",
       "             ('did', 402),\n",
       "             ('two', 186),\n",
       "             ('months', 108),\n",
       "             ('ago', 85),\n",
       "             ('time', 993),\n",
       "             ('wearing', 44),\n",
       "             ('jacket', 5),\n",
       "             ('as', 1945),\n",
       "             ('if', 1046),\n",
       "             ('toes', 12),\n",
       "             ('might', 172),\n",
       "             ('little', 932),\n",
       "             ('numb', 74),\n",
       "             ('from', 876),\n",
       "             ('cold', 137),\n",
       "             ('want', 802),\n",
       "             ('just', 1814),\n",
       "             ('drown', 3),\n",
       "             ('excitement', 20),\n",
       "             ('hype', 3),\n",
       "             ('still', 740),\n",
       "             ('very', 1086),\n",
       "             ('troubled', 63),\n",
       "             ('about', 2249),\n",
       "             ('where', 318),\n",
       "             ('both', 87),\n",
       "             ('america', 16),\n",
       "             ('world', 246),\n",
       "             ('are', 1104),\n",
       "             ('now', 878),\n",
       "             ('could', 567),\n",
       "             ('easily', 45),\n",
       "             ('describe', 34),\n",
       "             ('transformed', 2),\n",
       "             ('hopeless', 76),\n",
       "             ('but', 2790),\n",
       "             ('type', 52),\n",
       "             ('hopelessness', 5),\n",
       "             ('fought', 3),\n",
       "             ('trusting', 56),\n",
       "             ('who', 726),\n",
       "             ('patient', 12),\n",
       "             ('had', 898),\n",
       "             ('cool', 106),\n",
       "             ('confidence', 27),\n",
       "             ('ultimate', 5),\n",
       "             ('place', 177),\n",
       "             ('peace', 47),\n",
       "             ('divine', 70),\n",
       "             ('for', 3021),\n",
       "             ('worship', 5),\n",
       "             ('attain', 2),\n",
       "             ('not', 2303),\n",
       "             ('deprived', 56),\n",
       "             ('at', 1728),\n",
       "             ('although', 54),\n",
       "             ('do', 1312),\n",
       "             ('wake', 69),\n",
       "             ('up', 1165),\n",
       "             ('morning', 181),\n",
       "             ('cross', 17),\n",
       "             ('finish', 44),\n",
       "             ('line', 41),\n",
       "             ('exhausted', 99),\n",
       "             ('alive', 22),\n",
       "             ('exact', 11),\n",
       "             ('only', 427),\n",
       "             ('tried', 64),\n",
       "             ('three', 63),\n",
       "             ('can', 1200),\n",
       "             ('longing', 67),\n",
       "             ('came', 110),\n",
       "             ('wanting', 66),\n",
       "             ('child', 100),\n",
       "             ('extremely', 104),\n",
       "             ('honoured', 58),\n",
       "             ('have', 2803),\n",
       "             ('received', 25),\n",
       "             ('such', 245),\n",
       "             ('prestigious', 2),\n",
       "             ('award', 9),\n",
       "             ('never', 329),\n",
       "             ('been', 1056),\n",
       "             ('ashame', 2),\n",
       "             ('humiliated', 67),\n",
       "             ('life', 678),\n",
       "             ('said', 220),\n",
       "             ('wanted', 153),\n",
       "             ('give', 187),\n",
       "             ('you', 1878),\n",
       "             ('sample', 2),\n",
       "             ('writing', 141),\n",
       "             ('denied', 4),\n",
       "             ('then', 415),\n",
       "             ('m', 567),\n",
       "             ('generous', 118),\n",
       "             ('today', 407),\n",
       "             ('suppose', 51),\n",
       "             ('because', 1433),\n",
       "             ('share', 124),\n",
       "             ('one', 739),\n",
       "             ('taste', 24),\n",
       "             ('barely', 18),\n",
       "             ('speak', 45),\n",
       "             ('even', 717),\n",
       "             ('though', 365),\n",
       "             ('fine', 104),\n",
       "             ('make', 639),\n",
       "             ('called', 59),\n",
       "             ('sweet', 172),\n",
       "             ('jesus', 20),\n",
       "             ('sacrificed', 2),\n",
       "             ('everything', 262),\n",
       "             ('me', 2899),\n",
       "             ('would', 798),\n",
       "             ('resentful', 75),\n",
       "             ('toward', 56),\n",
       "             ('patrick', 3),\n",
       "             ('couldnt', 79),\n",
       "             ('read', 188),\n",
       "             ('her', 780),\n",
       "             ('nightly', 2),\n",
       "             ('books', 67),\n",
       "             ('pretty', 491),\n",
       "             ('scene', 20),\n",
       "             ('nurtured', 2),\n",
       "             ('respected', 69),\n",
       "             ('protected', 9),\n",
       "             ('equal', 11),\n",
       "             ('aa', 3),\n",
       "             ('meeting', 37),\n",
       "             ('really', 1182),\n",
       "             ('started', 238),\n",
       "             ('isolated', 75),\n",
       "             ('everyone', 154),\n",
       "             ('room', 90),\n",
       "             ('kind', 268),\n",
       "             ('insecure', 75),\n",
       "             ('here', 345),\n",
       "             ('anyways', 5),\n",
       "             ('back', 498),\n",
       "             ('remember', 227),\n",
       "             ('impatient', 58),\n",
       "             ('struggling', 28),\n",
       "             ('times', 216),\n",
       "             ('ok', 113),\n",
       "             ('blessings', 14),\n",
       "             ('upon', 47),\n",
       "             ('realized', 61),\n",
       "             ('don', 507),\n",
       "             ('t', 1156),\n",
       "             ('sharing', 34),\n",
       "             ('twitter', 17),\n",
       "             ('s', 602),\n",
       "             ('vain', 74),\n",
       "             ('thing', 234),\n",
       "             ('realize', 68),\n",
       "             ('met', 51),\n",
       "             ('none', 18),\n",
       "             ('those', 333),\n",
       "             ('goals', 17),\n",
       "             ('strongly', 44),\n",
       "             ('these', 446),\n",
       "             ('valuable', 69),\n",
       "             ('human', 53),\n",
       "             ('resources', 6),\n",
       "             ('actually', 239),\n",
       "             ('take', 258),\n",
       "             ('state', 41),\n",
       "             ('virtuous', 57),\n",
       "             ('cycle', 16),\n",
       "             ('development', 10),\n",
       "             ('growth', 11),\n",
       "             ('general', 39),\n",
       "             ('well', 344),\n",
       "             ('being', 849),\n",
       "             ('ed', 6),\n",
       "             ('tell', 198),\n",
       "             ('know', 1080),\n",
       "             ('shouldn', 21),\n",
       "             ('shamed', 30),\n",
       "             ('eating', 57),\n",
       "             ('protein', 6),\n",
       "             ('bar', 11),\n",
       "             ('breakfast', 20),\n",
       "             ('fact', 138),\n",
       "             ('ate', 21),\n",
       "             ('isn', 27),\n",
       "             ('what', 984),\n",
       "             ('makes', 303),\n",
       "             ('shameful', 2),\n",
       "             ('didn', 168),\n",
       "             ('hang', 20),\n",
       "             ('head', 112),\n",
       "             ('tuck', 6),\n",
       "             ('tail', 3),\n",
       "             ('eyes', 96),\n",
       "             ('many', 292),\n",
       "             ('turn', 70),\n",
       "             ('disgusted', 72),\n",
       "             ('by', 1108),\n",
       "             ('self', 145),\n",
       "             ('british', 10),\n",
       "             ('woman', 79),\n",
       "             ('admit', 96),\n",
       "             ('point', 161),\n",
       "             ('failure', 18),\n",
       "             ('began', 70),\n",
       "             ('curious', 77),\n",
       "             ('percieve', 2),\n",
       "             ('beneath', 8),\n",
       "             ('pride', 24),\n",
       "             ('why', 271),\n",
       "             ('regretful', 61),\n",
       "             ('horribly', 14),\n",
       "             ('butt', 10),\n",
       "             ('thank', 32),\n",
       "             ('gracious', 31),\n",
       "             ('punished', 83),\n",
       "             ('insulted', 76),\n",
       "             ('see', 440),\n",
       "             ('anyone', 157),\n",
       "             ('fashionable', 2),\n",
       "             ('shoe', 3),\n",
       "             ('icon', 2),\n",
       "             ('b', 28),\n",
       "             ('casual', 48),\n",
       "             ('every', 311),\n",
       "             ('other', 423),\n",
       "             ('night', 173),\n",
       "             ('when', 1689),\n",
       "             ('pretending', 6),\n",
       "             ('deserve', 29),\n",
       "             ('broke', 63),\n",
       "             ('how', 957),\n",
       "             ('proud', 104),\n",
       "             ('carried', 10),\n",
       "             ('out', 1024),\n",
       "             ('struggle', 29),\n",
       "             ('real', 128),\n",
       "             ('cant', 376),\n",
       "             ('help', 334),\n",
       "             ('perhaps', 47),\n",
       "             ('perception', 6),\n",
       "             ('isnt', 55),\n",
       "             ('keen', 39),\n",
       "             ('once', 155),\n",
       "             ('thought', 235),\n",
       "             ('faithful', 70),\n",
       "             ('enough', 295),\n",
       "             ('terrible', 84),\n",
       "             ('mrs', 3),\n",
       "             ('house', 110),\n",
       "             ('must', 135),\n",
       "             ('say', 416),\n",
       "             ('depressed', 116),\n",
       "             ('completely', 150),\n",
       "             ('meaningless', 5),\n",
       "             ('weird', 167),\n",
       "             ('nothing', 180),\n",
       "             ('applied', 3),\n",
       "             ('job', 118),\n",
       "             ('they', 855),\n",
       "             ('assured', 69),\n",
       "             ('exams', 15),\n",
       "             ('few', 243),\n",
       "             ('later', 57),\n",
       "             ('week', 225),\n",
       "             ('went', 149),\n",
       "             ('obtain', 7),\n",
       "             ('information', 30),\n",
       "             ('told', 125),\n",
       "             ('already', 196),\n",
       "             ('taken', 56),\n",
       "             ('pick', 37),\n",
       "             ('them', 801),\n",
       "             ('try', 208),\n",
       "             ('adventurous', 55),\n",
       "             ('sadness', 36),\n",
       "             ('phrases', 4),\n",
       "             ('sentences', 3),\n",
       "             ('themselves', 41),\n",
       "             ('wonderful', 114),\n",
       "             ('story', 87),\n",
       "             ('their', 452),\n",
       "             ('own', 251),\n",
       "             ('lively', 28),\n",
       "             ('compared', 11),\n",
       "             ('noisy', 3),\n",
       "             ('dead', 25),\n",
       "             ('atmosphere', 7),\n",
       "             ('down', 322),\n",
       "             ('hill', 9),\n",
       "             ('need', 477),\n",
       "             ('thoughts', 100),\n",
       "             ('feelings', 419),\n",
       "             ('play', 54),\n",
       "             ('anymore', 91),\n",
       "             ('anyway', 38),\n",
       "             ('irritable', 89),\n",
       "             ('too', 534),\n",
       "             ('rotten', 59),\n",
       "             ('put', 167),\n",
       "             ('happy', 311),\n",
       "             ('face', 137),\n",
       "             ('annoyed', 83),\n",
       "             ('lot', 287),\n",
       "             ('positive', 99),\n",
       "             ('future', 91),\n",
       "             ('virtual', 5),\n",
       "             ('birth', 19),\n",
       "             ('unit', 5),\n",
       "             ('education', 16),\n",
       "             ('message', 23),\n",
       "             ('lame', 58),\n",
       "             ('something', 630),\n",
       "             ('monday', 22),\n",
       "             ('friday', 24),\n",
       "             ('getting', 253),\n",
       "             ('decent', 10),\n",
       "             ('keep', 197),\n",
       "             ('starting', 128),\n",
       "             ('abused', 50),\n",
       "             ('coffee', 23),\n",
       "             ('machine', 9),\n",
       "             ('office', 27),\n",
       "             ('gonna', 34),\n",
       "             ('go', 493),\n",
       "             ('somewhere', 30),\n",
       "             ('cozy', 4),\n",
       "             ('get', 926),\n",
       "             ('lover', 8),\n",
       "             ('ecstatic', 50),\n",
       "             ('no', 561),\n",
       "             ('homework', 13),\n",
       "             ('honored', 67),\n",
       "             ('chosen', 18),\n",
       "             ('stand', 53),\n",
       "             ('sidelines', 3),\n",
       "             ('journey', 25),\n",
       "             ('his', 459),\n",
       "             ('cheering', 4),\n",
       "             ('him', 718),\n",
       "             ('watching', 73),\n",
       "             ('excel', 2),\n",
       "             ('grow', 25),\n",
       "             ('into', 477),\n",
       "             ('incredible', 13),\n",
       "             ('doctor', 16),\n",
       "             ('overwhelming', 15),\n",
       "             ('truly', 100),\n",
       "             ('blessed', 149),\n",
       "             ('understand', 132),\n",
       "             ('musicians', 3),\n",
       "             ('sometimes', 250),\n",
       "             ('inhibited', 34),\n",
       "             ('promotion', 4),\n",
       "             ('than', 557),\n",
       "             ('caring', 91),\n",
       "             ('greater', 16),\n",
       "             ('good', 481),\n",
       "             ('news', 41),\n",
       "             ('melancholy', 73),\n",
       "             ('were', 369),\n",
       "             ('always', 453),\n",
       "             ('entertained', 34),\n",
       "             ('simply', 70),\n",
       "             ('content', 89),\n",
       "             ('each', 156),\n",
       "             ('longer', 83),\n",
       "             ('angst', 2),\n",
       "             ('inside', 125),\n",
       "             ('passion', 16),\n",
       "             ('wronged', 59),\n",
       "             ('drove', 13),\n",
       "             ('initial', 10),\n",
       "             ('connection', 37),\n",
       "             ('emo', 16),\n",
       "             ('awkwardness', 4),\n",
       "             ('whenever', 58),\n",
       "             ('acceptable', 61),\n",
       "             ('before', 261),\n",
       "             ('lucky', 89),\n",
       "             ('guilty', 113),\n",
       "             ('having', 307),\n",
       "             ('got', 337),\n",
       "             ('without', 264),\n",
       "             ('serious', 19),\n",
       "             ('damage', 9),\n",
       "             ('disability', 3),\n",
       "             ('bad', 211),\n",
       "             ('people', 839),\n",
       "             ('drugs', 10),\n",
       "             ('trying', 233),\n",
       "             ('fill', 20),\n",
       "             ('void', 3),\n",
       "             ('hurt', 151),\n",
       "             ('past', 136),\n",
       "             ('drug', 5),\n",
       "             ('temporary', 5),\n",
       "             ('happiness', 34),\n",
       "             ('lost', 155),\n",
       "             ('fitness', 9),\n",
       "             ('during', 104),\n",
       "             ('weeks', 90),\n",
       "             ('either', 75),\n",
       "             ('awkward', 91),\n",
       "             ('valued', 74),\n",
       "             ('under', 66),\n",
       "             ('paid', 10),\n",
       "             ('worked', 43),\n",
       "             ('slave', 4),\n",
       "             ('unfortunately', 7),\n",
       "             ('personal', 43),\n",
       "             ('couldn', 35),\n",
       "             ('lock', 8),\n",
       "             ('deal', 57),\n",
       "             ('ways', 70),\n",
       "             ('may', 173),\n",
       "             ('passionate', 143),\n",
       "             ('taking', 114),\n",
       "             ('photos', 36),\n",
       "             ('pleased', 67),\n",
       "             ('addition', 7),\n",
       "             ('year', 227),\n",
       "             ('birds', 7),\n",
       "             ('decided', 90),\n",
       "             ('walk', 93),\n",
       "             ('around', 382),\n",
       "             ('fire', 10),\n",
       "             ('station', 4),\n",
       "             ('area', 36),\n",
       "             ('which', 468),\n",
       "             ('produced', 2),\n",
       "             ('love', 617),\n",
       "             ('aching', 74),\n",
       "             ('oh', 59),\n",
       "             ('things', 607),\n",
       "             ('front', 67),\n",
       "             ('sides', 11),\n",
       "             ('calves', 3),\n",
       "             ('muscles', 20),\n",
       "             ('fantasy', 5),\n",
       "             ('day', 540),\n",
       "             ('soon', 76),\n",
       "             ('featured', 2),\n",
       "             ('sports', 4),\n",
       "             ('swimsuit', 2),\n",
       "             ('model', 7),\n",
       "             ('maybe', 147),\n",
       "             ('special', 110),\n",
       "             ('issue', 31),\n",
       "             ('baby', 87),\n",
       "             ('he', 834),\n",
       "             ('needs', 65),\n",
       "             ('accepted', 146),\n",
       "             ('appreciated', 23),\n",
       "             ('adore', 7),\n",
       "             ('watches', 5),\n",
       "             ('gift', 31),\n",
       "             ('list', 56),\n",
       "             ('knows', 24),\n",
       "             ('unloved', 49),\n",
       "             ('surprises', 3),\n",
       "             ('kinda', 77),\n",
       "             ('cannot', 98),\n",
       "             ('series', 30),\n",
       "             ('unpleasant', 44),\n",
       "             ('stumbling', 2),\n",
       "             ('laid', 11),\n",
       "             ('bed', 95),\n",
       "             ('hide', 26),\n",
       "             ('girl', 120),\n",
       "             ('onto', 32),\n",
       "             ('top', 47),\n",
       "             ('gently', 4),\n",
       "             ('mommy', 10),\n",
       "             ('whats', 32),\n",
       "             ('bothering', 8),\n",
       "             ('sympathetic', 73),\n",
       "             ('narrator', 3),\n",
       "             ('struggled', 16),\n",
       "             ('through', 322),\n",
       "             ('low', 106),\n",
       "             ('income', 6),\n",
       "             ('think', 726),\n",
       "             ('work', 407),\n",
       "             ('there', 652),\n",
       "             ('mistake', 18),\n",
       "             ('doomed', 56),\n",
       "             ('garden', 14),\n",
       "             ('fabulous', 56),\n",
       "             ('immediately', 24),\n",
       "             ('guilt', 34),\n",
       "             ('selfish', 82),\n",
       "             ('practical', 6),\n",
       "             ('side', 76),\n",
       "             ('appears', 5),\n",
       "             ('care', 129),\n",
       "             ('kids', 102),\n",
       "             ('parents', 70),\n",
       "             ('disabilities', 2),\n",
       "             ('targeted', 4),\n",
       "             ('media', 31),\n",
       "             ('reports', 2),\n",
       "             ('sorrow', 11),\n",
       "             ('family', 220),\n",
       "             ('members', 28),\n",
       "             ('community', 42),\n",
       "             ('newtown', 2),\n",
       "             ('stunned', 25),\n",
       "             ('events', 24),\n",
       "             ('challenge', 26),\n",
       "             ('fun', 81),\n",
       "             ('way', 571),\n",
       "             ('knock', 3),\n",
       "             ('games', 21),\n",
       "             ('our', 344),\n",
       "             ('pressured', 69),\n",
       "             ('reach', 36),\n",
       "             ('any', 326),\n",
       "             ('certain', 68),\n",
       "             ('hand', 78),\n",
       "             ('its', 657),\n",
       "             ('warmth', 16),\n",
       "             ('sense', 127),\n",
       "             ('strange', 172),\n",
       "             ('sea', 15),\n",
       "             ('lives', 64),\n",
       "             ('shared', 20),\n",
       "             ('wave', 14),\n",
       "             ('hope', 172),\n",
       "             ('joy', 63),\n",
       "             ('surrender', 2),\n",
       "             ('surge', 2),\n",
       "             ('letting', 33),\n",
       "             ('wherever', 4),\n",
       "             ('breathing', 17),\n",
       "             ('delicate', 48),\n",
       "             ('dried', 4),\n",
       "             ('rose', 3),\n",
       "             ('inspired', 88),\n",
       "             ('motivation', 13),\n",
       "             ('needy', 57),\n",
       "             ('comfortable', 105),\n",
       "             ('vulnerable', 79),\n",
       "             ('secure', 12),\n",
       "             ('urge', 15),\n",
       "             ('cum', 2),\n",
       "             ('hard', 159),\n",
       "             ('relief', 16),\n",
       "             ('grumpy', 57),\n",
       "             ('hadnt', 18),\n",
       "             ('throughout', 28),\n",
       "             ('lungs', 8),\n",
       "             ('deciding', 5),\n",
       "             ('wasnt', 119),\n",
       "             ('unwell', 2),\n",
       "             ('felt', 326),\n",
       "             ('sitting', 82),\n",
       "             ('chest', 31),\n",
       "             ('flattened', 2),\n",
       "             ('wind', 18),\n",
       "             ('blow', 9),\n",
       "             ('presence', 38),\n",
       "             ('rest', 86),\n",
       "             ('honestly', 68),\n",
       "             ('victimized', 17),\n",
       "             ('rather', 204),\n",
       "             ('suggested', 5),\n",
       "             ('flower', 11),\n",
       "             ('distribution', 2),\n",
       "             ('esp', 3),\n",
       "             ('bought', 24),\n",
       "             ('flowers', 13),\n",
       "             ('didnt', 334),\n",
       "             ('bank', 11),\n",
       "             ('card', 19),\n",
       "             ('harder', 26),\n",
       "             ('careful', 5),\n",
       "             ('money', 79),\n",
       "             ('wondering', 38),\n",
       "             ('lesson', 15),\n",
       "             ('losing', 36),\n",
       "             ('paranoid', 69),\n",
       "             ('fuck', 28),\n",
       "             ('thinking', 159),\n",
       "             ('looking', 162),\n",
       "             ('un', 10),\n",
       "             ('mood', 52),\n",
       "             ('re', 112),\n",
       "             ('reading', 107),\n",
       "             ('post', 138),\n",
       "             ('reacting', 7),\n",
       "             ('find', 283),\n",
       "             ('shaking', 5),\n",
       "             ('imagine', 45),\n",
       "             ('someone', 324),\n",
       "             ('jealous', 73),\n",
       "             ('lonely', 109),\n",
       "             ('scared', 108),\n",
       "             ('protect', 14),\n",
       "             ('someones', 11),\n",
       "             ('lips', 22),\n",
       "             ('burning', 2),\n",
       "             ('havent', 92),\n",
       "             ('talked', 31),\n",
       "             ('days', 311),\n",
       "             ('bit', 642),\n",
       "             ('overwhelmed', 171),\n",
       "             ('different', 74),\n",
       "             ('we', 633),\n",
       "             ('mountain', 11),\n",
       "             ('whore', 3),\n",
       "             ('ashamed', 96),\n",
       "             ('bigger', 14),\n",
       "             ('dont', 596),\n",
       "             ('granted', 13),\n",
       "             ('shell', 8),\n",
       "             ('smarter', 6),\n",
       "             ('average', 12),\n",
       "             ('kid', 39),\n",
       "             ('shes', 66),\n",
       "             ('smart', 69),\n",
       "             ('satisfied', 84),\n",
       "             ('wish', 111),\n",
       "             ('possibly', 41),\n",
       "             ('forgotten', 13),\n",
       "             ('idiotic', 39),\n",
       "             ('last', 267),\n",
       "             ('years', 181),\n",
       "             ('whole', 158),\n",
       "             ('forgot', 14),\n",
       "             ('pack', 4),\n",
       "             ('shirts', 4),\n",
       "             ('joyful', 70),\n",
       "             ('sorry', 118),\n",
       "             ('benefit', 13),\n",
       "             ('strong', 115),\n",
       "             ('several', 42),\n",
       "             ('hours', 91),\n",
       "             ('playful', 40),\n",
       "             ('open', 74),\n",
       "             ('great', 136),\n",
       "             ('learned', 50),\n",
       "             ('month', 54),\n",
       "             ('us', 188),\n",
       "             ('manage', 21),\n",
       "             ('another', 164),\n",
       "             ('company', 33),\n",
       "             ('peaceful', 67),\n",
       "             ('boss', 12),\n",
       "             ('drunk', 11),\n",
       "             ('yell', 9),\n",
       "             ('unimportant', 66),\n",
       "             ('shit', 48),\n",
       "             ('together', 94),\n",
       "             ('stepped', 7),\n",
       "             ('private', 9),\n",
       "             ('club', 17),\n",
       "             ('standing', 22),\n",
       "             ('suit', 4),\n",
       "             ('convention', 4),\n",
       "             ('full', 92),\n",
       "             ('lethargic', 79),\n",
       "             ('incredibly', 66),\n",
       "             ('listless', 55),\n",
       "             ('least', 133),\n",
       "             ('weather', 36),\n",
       "             ('improving', 5),\n",
       "             ('tackling', 4),\n",
       "             ('exercise', 26),\n",
       "             ('backyard', 4),\n",
       "             ('shed', 13),\n",
       "             ('late', 42),\n",
       "             ('burden', 20),\n",
       "             ('system', 27),\n",
       "             ('intended', 12),\n",
       "             ('shopping', 21),\n",
       "             ('second', 64),\n",
       "             ('consecutive', 2),\n",
       "             ('safe', 103),\n",
       "             ('ourselves', 15),\n",
       "             ('trust', 66),\n",
       "             ('lingering', 2),\n",
       "             ('nor', 33),\n",
       "             ('becoming', 28),\n",
       "             ('apart', 22),\n",
       "             ('afraid', 134),\n",
       "             ('associate', 4),\n",
       "             ('regular', 16),\n",
       "             ('forget', 38),\n",
       "             ('important', 140),\n",
       "             ('ill', 192),\n",
       "             ('transfer', 3),\n",
       "             ('useful', 78),\n",
       "             ('next', 148),\n",
       "             ('class', 112),\n",
       "             ('website', 11),\n",
       "             ('specific', 18),\n",
       "             ('blogs', 20),\n",
       "             ('sites', 8),\n",
       "             ('rebellious', 54),\n",
       "             ('older', 31),\n",
       "             ('look', 272),\n",
       "             ('pop', 18),\n",
       "             ('music', 76),\n",
       "             ('childhood', 18),\n",
       "             ('fake', 63),\n",
       "             ('firmly', 5),\n",
       "             ('shouldnt', 32),\n",
       "             ('spend', 61),\n",
       "             ('fantastic', 55),\n",
       "             ('mixing', 2),\n",
       "             ('style', 56),\n",
       "             ('higher', 13),\n",
       "             ('end', 185),\n",
       "             ('items', 14),\n",
       "             ('convinced', 78),\n",
       "             ('twins', 2),\n",
       "             ('declare', 4),\n",
       "             ('upset', 63),\n",
       "             ('wont', 97),\n",
       "             ('greedy', 79),\n",
       "             ('sure', 302),\n",
       "             ('advantage', 9),\n",
       "             ('psychological', 3),\n",
       "             ('competition', 8),\n",
       "             ('hopelessly', 4),\n",
       "             ('burdened', 72),\n",
       "             ('big', 98),\n",
       "             ('jump', 11),\n",
       "             ('first', 226),\n",
       "             ('fight', 33),\n",
       "             ('saw', 53),\n",
       "             ('working', 110),\n",
       "             ('expecting', 10),\n",
       "             ('better', 268),\n",
       "             ('whack', 3),\n",
       "             ('messed', 7),\n",
       "             ('psychologically', 2),\n",
       "             ('fucked', 72),\n",
       "             ('underappreciated', 2),\n",
       "             ('stop', 157),\n",
       "             ('talking', 81),\n",
       "             ('uncomfortable', 84),\n",
       "             ('swarming', 2),\n",
       "             ('mouth', 23),\n",
       "             ('keeps', 25),\n",
       "             ('saying', 107),\n",
       "             ('unnecessary', 13),\n",
       "             ('word', 56),\n",
       "             ('thankful', 120),\n",
       "             ('everyday', 39),\n",
       "             ('burns', 2),\n",
       "             ('darkness', 6),\n",
       "             ('led', 11),\n",
       "             ('risen', 2),\n",
       "             ('show', 109),\n",
       "             ('wrong', 91),\n",
       "             ('kept', 43),\n",
       "             ('staring', 7),\n",
       "             ('violent', 75),\n",
       "             ('lapse', 4),\n",
       "             ('photography', 11),\n",
       "             ('vigorous', 8),\n",
       "             ('sun', 36),\n",
       "             ('racing', 4),\n",
       "             ('sky', 18),\n",
       "             ('relationship', 73),\n",
       "             ('opposite', 10),\n",
       "             ('based', 22),\n",
       "             ('drinking', 12),\n",
       "             ('socializing', 3),\n",
       "             ('personality', 16),\n",
       "             ('boring', 64),\n",
       "             ('obnoxious', 27),\n",
       "             ('after', 398),\n",
       "             ('workout', 20),\n",
       "             ('anything', 276),\n",
       "             ('handing', 4),\n",
       "             ('mormon', 2),\n",
       "             ('impressed', 75),\n",
       "             ('descriptions', 3),\n",
       "             ('seem', 88),\n",
       "             ('probably', 112),\n",
       "             ('defeated', 67),\n",
       "             ('nervous', 113),\n",
       "             ('planned', 7),\n",
       "             ('feed', 14),\n",
       "             ('younger', 17),\n",
       "             ('teens', 3),\n",
       "             ('bursting', 3),\n",
       "             ('delicious', 60),\n",
       "             ('again', 388),\n",
       "             ('four', 43),\n",
       "             ('sticking', 6),\n",
       "             ('sleeping', 38),\n",
       "             ('pattern', 11),\n",
       "             ('best', 124),\n",
       "             ('normal', 54),\n",
       "             ('perfect', 110),\n",
       "             ('fit', 35),\n",
       "             ('audiences', 5),\n",
       "             ('gorgeous', 73),\n",
       "             ('creativity', 12),\n",
       "             ('online', 24),\n",
       "             ('entertainment', 5),\n",
       "             ('hands', 56),\n",
       "             ('stupid', 133),\n",
       "             ('yelled', 4),\n",
       "             ('guess', 160),\n",
       "             ('making', 202),\n",
       "             ('definitive', 2),\n",
       "             ('decisions', 21),\n",
       "             ('truths', 6),\n",
       "             ('fearful', 57),\n",
       "             ('concerned', 19),\n",
       "             ('worried', 47),\n",
       "             ('loved', 153),\n",
       "             ('kicked', 11),\n",
       "             ('shoved', 2),\n",
       "             ('crappy', 69),\n",
       "             ('nowhere', 10),\n",
       "             ('near', 32),\n",
       "             ('closing', 4),\n",
       "             ('fan', 20),\n",
       "             ('london', 14),\n",
       "             ('grammar', 4),\n",
       "             ('collaboration', 2),\n",
       "             ('definitely', 69),\n",
       "             ('helps', 23),\n",
       "             ('creative', 88),\n",
       "             ('aware', 25),\n",
       "             ('indulge', 6),\n",
       "             ('wounded', 3),\n",
       "             ('loving', 112),\n",
       "             ('adult', 13),\n",
       "             ('charge', 4),\n",
       "             ('sudden', 15),\n",
       "             ('arrived', 15),\n",
       "             ('home', 270),\n",
       "             ('hot', 98),\n",
       "             ('sweaty', 5),\n",
       "             ('chinese', 10),\n",
       "             ('food', 81),\n",
       "             ('d', 64),\n",
       "             ('aside', 14),\n",
       "             ('quiet', 29),\n",
       "             ('person', 265),\n",
       "             ('loyal', 57),\n",
       "             ('clever', 63),\n",
       "             ('gt', 17),\n",
       "             ('avoided', 7),\n",
       "             ('gosh', 3),\n",
       "             ('honest', 40),\n",
       "             ('putting', 50),\n",
       "             ('splendid', 14),\n",
       "             ('naughty', 63),\n",
       "             ('intentions', 7),\n",
       "             ('ought', 13),\n",
       "             ('far', 114),\n",
       "             ('extent', 15),\n",
       "             ('currently', 26),\n",
       "             ('body', 143),\n",
       "             ('email', 15),\n",
       "             ('usually', 88),\n",
       "             ('contains', 3),\n",
       "             ('locate', 2),\n",
       "             ('jeff', 2),\n",
       "             ('therapeutic', 2),\n",
       "             ('helpful', 10),\n",
       "             ('keeping', 40),\n",
       "             ('blog', 173),\n",
       "             ('reluctant', 67),\n",
       "             ('whether', 74),\n",
       "             ('should', 405),\n",
       "             ('boyfriend', 34),\n",
       "             ('remind', 31),\n",
       "             ('fruit', 8),\n",
       "             ('won', 43),\n",
       "             ('disliked', 5),\n",
       "             ('most', 360),\n",
       "             ('lately', 125),\n",
       "             ('practice', 26),\n",
       "             ('present', 27),\n",
       "             ('living', 87),\n",
       "             ('appreciative', 47),\n",
       "             ('relaxed', 76),\n",
       "             ('benefits', 5),\n",
       "             ('budget', 6),\n",
       "             ('exotic', 2),\n",
       "             ('sets', 5),\n",
       "             ('paintings', 6),\n",
       "             ('running', 84),\n",
       "             ('loss', 45),\n",
       "             ('ever', 260),\n",
       "             ('dull', 79),\n",
       "             ('nostalgia', 8),\n",
       "             ('dad', 36),\n",
       "             ('luck', 11),\n",
       "             ('add', 43),\n",
       "             ('input', 7),\n",
       "             ('advice', 16),\n",
       "             ('guidance', 7),\n",
       "             ('enjoy', 92),\n",
       "             ('hearing', 26),\n",
       "             ('faith', 28),\n",
       "             ('stories', 39),\n",
       "             ('believers', 2),\n",
       "             ('leave', 128),\n",
       "             ('inadequate', 65),\n",
       "             ('worthwhile', 65),\n",
       "             ('depth', 14),\n",
       "             ('city', 29),\n",
       "             ('sized', 4),\n",
       "             ('escape', 17),\n",
       "             ('tia', 2),\n",
       "             ('held', 18),\n",
       "             ('reaching', 10),\n",
       "             ('potential', 17),\n",
       "             ('decide', 37),\n",
       "             ('pray', 28),\n",
       "             ('reason', 119),\n",
       "             ('lol', 27),\n",
       "             ('amvassago', 2),\n",
       "             ('laughing', 16),\n",
       "             ('beefy', 2),\n",
       "             ('epic', 5),\n",
       "             ('beef', 3),\n",
       "             ('cake', 10),\n",
       "             ('ahead', 34),\n",
       "             ('hopeful', 74),\n",
       "             ('long', 206),\n",
       "             ('sticks', 3),\n",
       "             ('useless', 83),\n",
       "             ('pointless', 5),\n",
       "             ('mirror', 9),\n",
       "             ('partly', 14),\n",
       "             ('worry', 40),\n",
       "             ('reputation', 6),\n",
       "             ('positivity', 2),\n",
       "             ('doing', 312),\n",
       "             ('slightly', 116),\n",
       "             ('productive', 68),\n",
       "             ('zero', 7),\n",
       "             ('sending', 8),\n",
       "             ...])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8404"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = len(tokenizer.word_index) + 1\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_length = max(len(sequence) for sequence in tokenizer.texts_to_sequences(df['text_clean']))\n",
    "input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 51, 128)           1075712   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 64)                37248     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1113350 (4.25 MB)\n",
      "Trainable params: 1113350 (4.25 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=128, input_length=input_length))\n",
    "model.add(GRU(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model consists of three layers:\n",
    "\n",
    "- Embedding Layer: Maps the input sequences to dense vectors of fixed size (128).\n",
    "- GRU (Gated Recurrent Unit) Layer: A type of recurrent layer with 64 units and dropout of 0.2 for regularization.\n",
    "- Dense Layer: The output layer with 6 units (for each emotion category) and softmax activation.\n",
    "\n",
    "The model is compiled using the Adam optimizer and categorical cross-entropy loss, with accuracy as the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "To train the text classification model, the preprocessed data is converted into sequences and padded/truncated to a fixed length. The training process is then executed using the Keras `fit` method.\n",
    "\n",
    "The model undergoes training for 10 epochs, with a batch size of 32. The training progress is displayed in terms of loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(df['text_clean'])\n",
    "\n",
    "X_train = pad_sequences(sequences, maxlen=input_length, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 51)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 20s 40ms/step - loss: 0.0653 - accuracy: 0.9754 - val_loss: 0.3773 - val_accuracy: 0.8938\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0569 - accuracy: 0.9791 - val_loss: 0.3882 - val_accuracy: 0.8850\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 19s 39ms/step - loss: 0.0470 - accuracy: 0.9828 - val_loss: 0.4354 - val_accuracy: 0.8835\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 0.0400 - accuracy: 0.9862 - val_loss: 0.4324 - val_accuracy: 0.8873\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 21s 42ms/step - loss: 0.0350 - accuracy: 0.9874 - val_loss: 0.4720 - val_accuracy: 0.8865\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 19s 39ms/step - loss: 0.0306 - accuracy: 0.9881 - val_loss: 0.4629 - val_accuracy: 0.8855\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 22s 43ms/step - loss: 0.0261 - accuracy: 0.9896 - val_loss: 0.5071 - val_accuracy: 0.8840\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 19s 38ms/step - loss: 0.0253 - accuracy: 0.9908 - val_loss: 0.5134 - val_accuracy: 0.8867\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 18s 36ms/step - loss: 0.0242 - accuracy: 0.9904 - val_loss: 0.4902 - val_accuracy: 0.8808\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 18s 36ms/step - loss: 0.0218 - accuracy: 0.9915 - val_loss: 0.5138 - val_accuracy: 0.8863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x141d15fd0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(X_train, one_hot_labels, epochs=epochs, batch_size=batch_size, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training results provide insights into the model's convergence, with both training and validation accuracies and losses over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 85ms/step\n",
      "Predicted Label: joy\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Predicted Label: joy\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "Predicted Label: joy\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted Label: joy\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted Label: joy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_label(model, tokenizer, input_text, input_length):\n",
    "    sequence = tokenizer.texts_to_sequences([input_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=input_length, padding='post', truncating='post')\n",
    "    predicted_probabilities = model.predict(padded_sequence)\n",
    "    predicted_label = label_encoder.inverse_transform([np.argmax(predicted_probabilities)])\n",
    "\n",
    "    return predicted_label[0]\n",
    "\n",
    "a = [\"I\", \"You\", \"He\", \"She\", \"They\"]\n",
    "\n",
    "for i in a:\n",
    "    predicted_label = predict_label(model, tokenizer, i.lower() + \"very happy\", input_length)\n",
    "    print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO viz metrics and split test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rows of Interest\n",
    "\n",
    "| Row  | Text | Label |\n",
    "| ---  | ---- | ----- |\n",
    "| 5578 | i now don t want to feel slutty | love\n",
    "|12380 | i gotta feeling da bul taewuhbeoryeo burn it up i gotta feeling niga ulbujitneun nal ah neoneun wiheomhae gal ttaekkaji gatsseo get away woooo becuz i m cuz i m dangerous b | anger\n",
    "| 16503 | i do not feel insecure or unsafe | fear\n",
    "| 18330 | i more important than going fun ipad strategy games original boots from ugg wear ugg boots this winter low cost ugg boots uggs need to get washed inside they are also lightweight so you won t feel burdened with them speed up finances with payday loans payday loans the monthly solution for you | sadness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
